"""
过滤流水线进度信息汇总模块

本模块用于汇总和查询舆情分析系统过滤（filter）流水线的进度信息，便于前端展示、任务监控和进度上报，主要功能包括：

1. 统计各渠道过滤进度，包括总数、已完成、失败、保留等明细，支持多渠道聚合。
2. 读取过滤流水线的进度文件和汇总文件，支持断点续跑和异常容错。
3. 提供最近处理记录、无关样本等摘要信息，便于前端展示和问题排查。
4. 支持判断过滤任务是否正在运行，结合内存状态与进度文件。
5. 适用于后端接口、管理后台、前端进度轮询等场景。

主要导出函数：
- collect_filter_status：聚合过滤进度与摘要信息
- count_jsonl_rows：统计JSONL文件行数
- load_filter_summary_data：读取过滤流水线汇总数据

适用场景：
- 过滤任务进度监控与展示
- 后端接口与前端进度轮询
- 任务调度与异常排查
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, List

from src.utils.setting.paths import bucket  # type: ignore

from .filter_jobs import is_filter_job_running
from .paths import FILTER_PROGRESS_DIR, FILTER_SUMMARY_FILENAME

_RECENT_RECORD_LIMIT = 40
_IRRELEVANT_SAMPLE_LIMIT = 10


def count_jsonl_rows(path: Path) -> int:
    """Return the number of non-empty rows inside a JSONL file."""

    if not path.exists():
        return 0
    try:
        with path.open("r", encoding="utf-8") as fh:
            return sum(1 for line in fh if line.strip())
    except Exception:  # pragma: no cover - defensive for partial files
        return 0


def load_filter_summary_data(topic: str, date: str) -> Dict[str, Any]:
    """Read the persisted summary generated by the filter pipeline."""

    filter_dir = bucket("filter", topic, date)
    summary_path = filter_dir / FILTER_SUMMARY_FILENAME
    payload: Dict[str, Any] = {}
    if summary_path.exists():
        try:
            with summary_path.open("r", encoding="utf-8") as fh:
                raw = json.load(fh) or {}
            if isinstance(raw, dict):
                payload = raw
        except Exception:
            payload = {}

    summary = {
        "topic": topic,
        "date": date,
        "total_rows": int(payload.get("total_rows") or 0),
        "kept_rows": int(payload.get("kept_rows") or 0),
        "discarded_rows": int(payload.get("discarded_rows") or 0),
        "token_usage": int(payload.get("token_usage") or 0),
        "irrelevant_samples": payload.get("irrelevant_samples")
        if isinstance(payload.get("irrelevant_samples"), list)
        else [],
        "completed": bool(payload.get("completed")),
        "updated_at": payload.get("updated_at"),
    }
    return summary


def collect_filter_status(topic: str, date: str) -> Dict[str, Any]:
    """Aggregate per-channel filter progress information."""

    clean_dir = bucket("clean", topic, date)
    filter_dir = bucket("filter", topic, date)

    channels: List[Dict[str, Any]] = []
    combined_recent: List[Dict[str, Any]] = []

    total_rows = 0
    completed_rows = 0
    failed_rows = 0
    kept_rows = 0
    running = False

    clean_files = sorted(clean_dir.glob("*.jsonl")) if clean_dir.exists() else []
    for file_path in clean_files:
        channel = file_path.stem
        if channel == "all":
            continue

        progress_path = FILTER_PROGRESS_DIR / f"{topic}_{date}_{channel}_progress.json"
        progress_data: Dict[str, Any] = {}
        if progress_path.exists():
            try:
                with progress_path.open("r", encoding="utf-8") as fh:
                    raw = json.load(fh) or {}
                if isinstance(raw, dict):
                    progress_data = raw
            except Exception:
                progress_data = {}

        channel_total = progress_data.get("total_count")
        if not isinstance(channel_total, int):
            channel_total = count_jsonl_rows(file_path)

        channel_completed = progress_data.get("completed_count")
        if not isinstance(channel_completed, int):
            channel_completed = len(progress_data.get("completed_indices", []))

        channel_failed = progress_data.get("failed_count")
        if not isinstance(channel_failed, int):
            channel_failed = len(progress_data.get("failed_indices", []))

        channel_kept = count_jsonl_rows(filter_dir / f"{channel}.jsonl")

        total_rows += channel_total
        completed_rows += min(channel_total, channel_completed)
        failed_rows += min(channel_total, channel_failed)
        kept_rows += channel_kept

        recent_items = progress_data.get("recent_records") or []
        if isinstance(recent_items, list):
            combined_recent.extend(item for item in recent_items if isinstance(item, dict))

        channel_running = progress_path.exists() and (channel_total == 0 or channel_completed < channel_total)
        running = running or channel_running

        channels.append({
            "channel": channel,
            "total": channel_total,
            "completed": channel_completed,
            "failed": channel_failed,
            "kept": channel_kept,
            "updated_at": progress_data.get("updated_at"),
        })

    summary = load_filter_summary_data(topic, date)
    if not summary["total_rows"]:
        summary["total_rows"] = total_rows
    if not summary["kept_rows"]:
        summary["kept_rows"] = kept_rows
    summary["discarded_rows"] = max(summary["total_rows"] - summary["kept_rows"], 0)

    combined_recent.sort(key=lambda item: item.get("updated_at") or "", reverse=True)
    recent_records = combined_recent[:_RECENT_RECORD_LIMIT]
    irrelevant_samples = (summary.get("irrelevant_samples") or [])[:_IRRELEVANT_SAMPLE_LIMIT]

    token_usage = int(summary.get("token_usage") or 0)

    progress_overview = {
        "total": total_rows,
        "completed": completed_rows,
        "failed": failed_rows,
        "kept": kept_rows,
        "percentage": (completed_rows / total_rows * 100) if total_rows else 0,
        "token_usage": token_usage,
    }

    running = running or is_filter_job_running(topic, date)

    return {
        "topic": topic,
        "date": date,
        "running": running,
        "channels": channels,
        "recent_records": recent_records,
        "summary": summary,
        "progress": progress_overview,
        "irrelevant_samples": irrelevant_samples,
    }


__all__ = [
    "collect_filter_status",
    "count_jsonl_rows",
    "load_filter_summary_data",
]
